{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:30:37.970878Z",
     "start_time": "2022-06-02T02:30:34.740917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Steven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Steven\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import helper_data, helper_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2023\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.8\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "SEQUENCE_LENGTH = 300\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    {\n",
    "        'name': \"training.1600000.processed.noemoticon.csv\",\n",
    "        'api': \"kazanova/sentiment140\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/kazanova/sentiment140\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"reddit_wsb.csv\",\n",
    "        'api': \"gpreda/reddit-wallstreetsbets-posts\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_data.csv\",\n",
    "        'api': \"yash612/stockmarket-sentiment-dataset\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_tweets.csv\",\n",
    "        'api': \"equinxx/stock-tweets-for-sentiment-analysis-and-prediction\",\n",
    "        'location': \"data/unorganized/Stock Tweets for Sentiment Analysis and Prediction\",\n",
    "        'url': \"https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"Company_Tweet.csv\",\n",
    "        'api': \"omermetinn/tweets-about-the-top-companies-from-2015-to-2020\",\n",
    "        'location': \"data/unorganized/Tweets about the Top Companies from 2015 to 2020\",\n",
    "        'url': \"https://www.kaggle.com/datasets/omermetinn/tweets-about-the-top-companies-from-2015-to-2020\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stockerbot-export.csv\",\n",
    "        'api': \"davidwallach/financial-tweets\",\n",
    "        'location': \"data/unorganized/Financial Tweets\",\n",
    "        'url': \"https://www.kaggle.com/datasets/davidwallach/financial-tweets\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for dataset_info in DATASETS:\n",
    "    dataset_name = dataset_info['name']\n",
    "    dataset_location = dataset_info['location']\n",
    "\n",
    "    if not os.path.exists(os.path.join(dataset_info['location'], dataset_name)):\n",
    "        print(f\"Downloading {dataset_name} from {dataset_info['url']} to {dataset_location}...\")\n",
    "        kaggle.api.dataset_download_files(dataset_info['api'], path=dataset_location, unzip=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"sentiment.csv\" from https://github.com/surge-ai/stock-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:06.431016Z",
     "start_time": "2022-06-02T02:54:06.391735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    3685\n",
      "0    2106\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "1    327\n",
      "0    173\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_filename = {\n",
    "    '0': (\"training.1600000.processed.noemoticon.csv\", [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]),\n",
    "    '1': (\"stock_data.csv\", [\"text\", \"target\"]),\n",
    "    '2': (\"sentiment.csv\", [\"Stock Ticker\", \"Tweet Text\", \"Sentiment\", \"Tweet URL\"])\n",
    "}\n",
    "\n",
    "# dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"0\"][0])\n",
    "# df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"0\"][1])\n",
    "# print(df['target'].value_counts())\n",
    "\n",
    "dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"1\"][0])\n",
    "train_df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"1\"][1], skiprows=1)\n",
    "train_df['target'] = train_df['target'].replace({-1: 0, 1: 1})\n",
    "print(train_df['target'].value_counts())\n",
    "\n",
    "dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"2\"][0])\n",
    "test_df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"2\"][1], skiprows=1)\n",
    "test_df.rename(columns={\"Sentiment\": \"target\"}, inplace=True)\n",
    "test_df['target'] = test_df['target'].replace({'Negative': 0, 'Positive': 1})\n",
    "test_df = test_df[['target', 'Tweet Text']]\n",
    "test_df.rename(columns={'Tweet Text': 'text'}, inplace=True)\n",
    "print(test_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time too long, not try\n",
    "# augmented_train_df = helper_data.augment_text_with_parallel_back_translation(train_df, n=1)\n",
    "# print(augmented_train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    7370\n",
      "0    4212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "augmented_train_df = helper_data.augment_text_with_synonyms(train_df, n=1)\n",
    "print(augmented_train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kernel crash\n",
    "# augmented_train_df = helper_data.augment_text_with_paraphrasing(train_df, n=1)\n",
    "# print(augmented_train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    14787\n",
      "0     8377\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "resampled_train_df = helper_data.resample_data(augmented_train_df, n=2)  # Create 2 bootstrapped samples\n",
    "print(resampled_train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    16754\n",
      "1    14787\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "oversampled_train_df = helper_data.oversample_data(resampled_train_df, n=1)  # Create 1 oversampled set\n",
    "print(oversampled_train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_train_df.to_csv('data/train_data.csv', index=False)\n",
    "train_df = pd.read_csv('data/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_mapping = {0: 0, 4: 1}\n",
    "# test_df['target'] = test_df['target'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25232, 2) (6309, 2) (500, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(tokenizer, texts, labels, batch_size=32):\n",
    "    texts = texts.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "    input_ids = tokenized_texts[\"input_ids\"]\n",
    "    attention_mask = tokenized_texts[\"attention_mask\"]\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
    "\n",
    "train_dataloader = preprocess_and_tokenize(tokenizer, train_df['text'], train_df['target'])\n",
    "val_dataloader = preprocess_and_tokenize(tokenizer, val_df['text'], val_df['target'])\n",
    "test_dataloader = preprocess_and_tokenize(tokenizer, test_df['text'], test_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Cornell\\course\\CS6386\\Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements\\Sentiment_Analysis\\finetune.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/finetune.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain', num_labels=2)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/finetune.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# model.to(device) \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/finetune.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/finetune.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39myiyanghkust/finbert-tone\u001b[39;49m\u001b[39m'\u001b[39;49m,num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/finetune.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mto(device) \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/finetune.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39myiyanghkust/finbert-tone\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\modeling_utils.py:3307\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3297\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3298\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3300\u001b[0m     (\n\u001b[0;32m   3301\u001b[0m         model,\n\u001b[0;32m   3302\u001b[0m         missing_keys,\n\u001b[0;32m   3303\u001b[0m         unexpected_keys,\n\u001b[0;32m   3304\u001b[0m         mismatched_keys,\n\u001b[0;32m   3305\u001b[0m         offload_index,\n\u001b[0;32m   3306\u001b[0m         error_msgs,\n\u001b[1;32m-> 3307\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[0;32m   3308\u001b[0m         model,\n\u001b[0;32m   3309\u001b[0m         state_dict,\n\u001b[0;32m   3310\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3311\u001b[0m         resolved_archive_file,\n\u001b[0;32m   3312\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3313\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   3314\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[0;32m   3315\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[0;32m   3316\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[0;32m   3317\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   3318\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   3319\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m   3320\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[0;32m   3321\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[0;32m   3322\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   3323\u001b[0m     )\n\u001b[0;32m   3325\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[0;32m   3326\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\modeling_utils.py:3756\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3752\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[0;32m   3753\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m   3754\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3755\u001b[0m         )\n\u001b[1;32m-> 3756\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m is_quantized:\n\u001b[0;32m   3759\u001b[0m     unexpected_keys \u001b[39m=\u001b[39m [elem \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m unexpected_keys \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m elem]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain', num_labels=2)\n",
    "# model.to(device) \n",
    "# tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=2)\n",
    "model.to(device) \n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as t:\n",
    "        for batch in t:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, unit=\"batch\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predicted_class = torch.round(torch.sigmoid(logits)).cpu().numpy()\n",
    "            val_predictions.extend(predicted_class)\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_predictions = [int(round(val[0])) for val in val_predictions]\n",
    "\n",
    "    accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    precision = precision_score(val_targets, val_predictions)\n",
    "    recall = recall_score(val_targets, val_predictions)\n",
    "    f1 = f1_score(val_targets, val_predictions)\n",
    "    roc_auc = roc_auc_score(val_targets, val_predictions)\n",
    "\n",
    "    print(f\"Validation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        model.save_pretrained('best_model')\n",
    "        tokenizer.save_pretrained('best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change folder 'finetuned_model' from https://github.com/yya518/FinBERT/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at best_model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "best_model = BertForSequenceClassification.from_pretrained('best_model')\n",
    "best_model.to(device)  \n",
    "best_tokenizer = BertTokenizer.from_pretrained('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:26<00:00,  2.33s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Accuracy: 0.3865, Precision: 0.6479, Recall: 0.0628, F1 Score: 0.1146, ROC AUC: 0.5021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_on_test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, unit=\"batch\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predicted_class = torch.round(torch.sigmoid(logits)).cpu().numpy()\n",
    "            val_predictions.extend(predicted_class)\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_predictions = [int(round(val[0])) for val in val_predictions]\n",
    "\n",
    "    accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    precision = precision_score(val_targets, val_predictions)\n",
    "    recall = recall_score(val_targets, val_predictions)\n",
    "    f1 = f1_score(val_targets, val_predictions)\n",
    "    roc_auc = roc_auc_score(val_targets, val_predictions)\n",
    "\n",
    "    print(f\"Validation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "evaluate_model_on_test(best_model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
