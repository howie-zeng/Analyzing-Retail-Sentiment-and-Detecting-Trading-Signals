{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:30:37.970878Z",
     "start_time": "2022-06-02T02:30:34.740917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:44:18.286115Z",
     "start_time": "2022-06-02T02:44:18.222850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2023\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.8\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "SEQUENCE_LENGTH = 300\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    {\n",
    "        'name': \"training.1600000.processed.noemoticon.csv\",\n",
    "        'api': \"kazanova/sentiment140\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/kazanova/sentiment140\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"reddit_wsb.csv\",\n",
    "        'api': \"gpreda/reddit-wallstreetsbets-posts\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_data.csv\",\n",
    "        'api': \"yash612/stockmarket-sentiment-dataset\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_tweets.csv\",\n",
    "        'api': \"equinxx/stock-tweets-for-sentiment-analysis-and-prediction\",\n",
    "        'location': \"data/unorganized/Stock Tweets for Sentiment Analysis and Prediction\",\n",
    "        'url': \"https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"Company_Tweet.csv\",\n",
    "        'api': \"omermetinn/tweets-about-the-top-companies-from-2015-to-2020\",\n",
    "        'location': \"data/unorganized/Tweets about the Top Companies from 2015 to 2020\",\n",
    "        'url': \"https://www.kaggle.com/datasets/omermetinn/tweets-about-the-top-companies-from-2015-to-2020\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stockerbot-export.csv\",\n",
    "        'api': \"davidwallach/financial-tweets\",\n",
    "        'location': \"data/unorganized/Financial Tweets\",\n",
    "        'url': \"https://www.kaggle.com/datasets/davidwallach/financial-tweets\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for dataset_info in DATASETS:\n",
    "    dataset_name = dataset_info['name']\n",
    "    dataset_location = dataset_info['location']\n",
    "\n",
    "    if not os.path.exists(os.path.join(dataset_info['location'], dataset_name)):\n",
    "        print(f\"Downloading {dataset_name} from {dataset_info['url']} to {dataset_location}...\")\n",
    "        kaggle.api.dataset_download_files(dataset_info['api'], path=dataset_location, unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:06.431016Z",
     "start_time": "2022-06-02T02:54:06.391735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "4    3685\n",
      "0    2106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_filename = {\n",
    "    '0': (\"training.1600000.processed.noemoticon.csv\", [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]),\n",
    "    '1': (\"stock_data.csv\", [\"text\", \"target\"])\n",
    "}\n",
    "\n",
    "dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"0\"][0])\n",
    "df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"0\"][1])\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "test_dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"1\"][0])\n",
    "test_df = pd.read_csv(test_dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"1\"][1], skiprows=1)\n",
    "test_df['target'] = test_df['target'].replace({-1: 0, 1: 4})\n",
    "print(test_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0: 0, 4: 1}\n",
    "test_df['target'] = test_df['target'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4689, 2) (580, 2) (522, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test, = train_test_split(test_df, stratify=test_df['target'], test_size=0.1, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, stratify=df_train['target'],test_size=0.1, random_state=42)\n",
    "print(df_train.shape, df_test.shape, df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(tokenizer, texts, labels, batch_size=32):\n",
    "    texts = texts.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "    input_ids = tokenized_texts[\"input_ids\"]\n",
    "    attention_mask = tokenized_texts[\"attention_mask\"]\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
    "\n",
    "train_dataloader = preprocess_and_tokenize(tokenizer, df_train['text'], df_train['target'])\n",
    "val_dataloader = preprocess_and_tokenize(tokenizer, df_val['text'], df_val['target'])\n",
    "test_dataloader = preprocess_and_tokenize(tokenizer, df_test['text'], df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 147/147 [19:11<00:00,  7.83s/batch, loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.5451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:38<00:00,  2.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Accuracy: 0.2261, Precision: 0.2429, Recall: 0.1024, F1 Score: 0.1441, ROC AUC: 0.2723\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain', num_labels=2) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 1\n",
    "best_f1 = 0.0  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as t:\n",
    "        for batch in t:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, unit=\"batch\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predicted_class = torch.round(torch.sigmoid(logits)).cpu().numpy()\n",
    "            val_predictions.extend(predicted_class)\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_predictions = [int(round(val[0])) for val in val_predictions]\n",
    "\n",
    "    accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    precision = precision_score(val_targets, val_predictions)\n",
    "    recall = recall_score(val_targets, val_predictions)\n",
    "    f1 = f1_score(val_targets, val_predictions)\n",
    "    roc_auc = roc_auc_score(val_targets, val_predictions)\n",
    "\n",
    "    print(f\"Validation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        model.save_pretrained('best_model')\n",
    "        tokenizer.save_pretrained('best_model')\n",
    "\n",
    "best_model = BertForSequenceClassification.from_pretrained('best_model')\n",
    "best_tokenizer = BertTokenizer.from_pretrained('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing - Accuracy: 0.2690, Precision: 0.2993, Recall: 0.1111, F1 Score: 0.1621, ROC AUC: 0.3281\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_on_test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_class = torch.round(torch.sigmoid(logits)).cpu().numpy()\n",
    "            test_predictions.extend(predicted_class)\n",
    "            test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_predictions = [int(round(val[0])) for val in test_predictions]\n",
    "\n",
    "    accuracy = accuracy_score(test_targets, test_predictions)\n",
    "    precision = precision_score(test_targets, test_predictions)\n",
    "    recall = recall_score(test_targets, test_predictions)\n",
    "    f1 = f1_score(test_targets, test_predictions)\n",
    "    roc_auc = roc_auc_score(test_targets, test_predictions)\n",
    "\n",
    "    print(f\"Testing - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "evaluate_model_on_test(best_model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
