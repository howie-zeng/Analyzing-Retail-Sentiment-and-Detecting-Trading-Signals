{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible datasets\n",
    " - https://data.world/crowdflower/apple-twitter-sentiment\n",
    " - https://paperswithcode.com/dataset/stocknet-1\n",
    " - https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction\n",
    " - https://www.kaggle.com/datasets/thedevastator/tweet-sentiment-s-impact-on-stock-returns\n",
    " - https://ieee-dataport.org/open-access/stock-market-tweets-data\n",
    " - https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset\n",
    " - https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import kaggle\n",
    "\n",
    "import requests\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2023\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.8\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "SEQUENCE_LENGTH = 300\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "W2V_SIZE = 300\n",
    "NUM_CORE = multiprocessing.cpu_count()\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    {\n",
    "        'name': \"training.1600000.processed.noemoticon.csv\",\n",
    "        'api': \"kazanova/sentiment140\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/kazanova/sentiment140\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"reddit_wsb.csv\",\n",
    "        'api': \"gpreda/reddit-wallstreetsbets-posts\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_data.csv\",\n",
    "        'api': \"yash612/stockmarket-sentiment-dataset\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_tweets.csv\",\n",
    "        'api': \"equinxx/stock-tweets-for-sentiment-analysis-and-prediction\",\n",
    "        'location': \"data/unorganized/Stock Tweets for Sentiment Analysis and Prediction\",\n",
    "        'url': \"https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"Company_Tweet.csv\",\n",
    "        'api': \"omermetinn/tweets-about-the-top-companies-from-2015-to-2020\",\n",
    "        'location': \"data/unorganized/Tweets about the Top Companies from 2015 to 2020\",\n",
    "        'url': \"https://www.kaggle.com/datasets/omermetinn/tweets-about-the-top-companies-from-2015-to-2020\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stockerbot-export.csv\",\n",
    "        'api': \"davidwallach/financial-tweets\",\n",
    "        'location': \"data/unorganized/Financial Tweets\",\n",
    "        'url': \"https://www.kaggle.com/datasets/davidwallach/financial-tweets\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for dataset_info in DATASETS:\n",
    "    dataset_name = dataset_info['name']\n",
    "    dataset_location = dataset_info['location']\n",
    "\n",
    "    if not os.path.exists(os.path.join(dataset_info['location'], dataset_name)):\n",
    "        print(f\"Downloading {dataset_name} from {dataset_info['url']} to {dataset_location}...\")\n",
    "        kaggle.api.dataset_download_files(dataset_info['api'], path=dataset_location, unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "4    3685\n",
      "0    2106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_filename = {\n",
    "    '0': (\"training.1600000.processed.noemoticon.csv\", [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]),\n",
    "    '1': (\"stock_data.csv\", [\"text\", \"target\"])\n",
    "}\n",
    "\n",
    "dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"0\"][0])\n",
    "df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"0\"][1])\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "test_dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"1\"][0])\n",
    "test_df = pd.read_csv(test_dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"1\"][1], skiprows=1)\n",
    "test_df['target'] = test_df['target'].replace({-1: 0, 1: 4})\n",
    "print(test_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...       4\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...       4\n",
       "2  user I'd be afraid to short AMZN - they are lo...       4\n",
       "3                                  MNTA Over 12.00         4\n",
       "4                                   OI  Over 21.37         4\n",
       "5                                  PGNX  Over 3.04         4\n",
       "6  AAP - user if so then the current downtrend wi...       0\n",
       "7  Monday's relative weakness. NYX WIN TIE TAP IC...       0\n",
       "8  GOOG - ower trend line channel test & volume s...       4\n",
       "9             AAP will watch tomorrow for ONG entry.       4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text):\n",
    "#     text = text.lower()\n",
    "\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text) # remove special characters\n",
    "\n",
    "#     stop_words = set(stopwords.words('english')) # tokenization and remove stopwords\n",
    "#     words = text.split()\n",
    "#     words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "#     stemmer = PorterStemmer()\n",
    "#     words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "#     return ' '.join(words)\n",
    "\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    if stem:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess)\n",
    "# test_df['text'] = test_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>awww bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>dived many times ball managed save 50 rest go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>need hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>hey long time see yes rains bit bit lol fine t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>nope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>que muera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_       awww bummer shoulda got david carr third day  \n",
       "1    scotthamilton  upset update facebook texting might cry result...  \n",
       "2         mattycus  dived many times ball managed save 50 rest go ...  \n",
       "3          ElleCTF                   whole body feels itchy like fire  \n",
       "4           Karoli                                   behaving mad see  \n",
       "5         joy_wolf                                         whole crew  \n",
       "6          mybirch                                           need hug  \n",
       "7             coZZ  hey long time see yes rains bit bit lol fine t...  \n",
       "8  2Hood4Hollywood                                               nope  \n",
       "9          mimismo                                          que muera  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.75472656 0.75392969 0.75528125 0.75476562 0.75523828]\n",
      "Mean CV Accuracy: 75.48%\n",
      "Accuracy: 75.53%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76    159494\n",
      "           4       0.76      0.75      0.75    160506\n",
      "\n",
      "    accuracy                           0.76    320000\n",
      "   macro avg       0.76      0.76      0.76    320000\n",
      "weighted avg       0.76      0.76      0.76    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "alpha = 1  # Laplace smoothing parameter (add-one smoothing), regularization\n",
    "clf = MultinomialNB(alpha=alpha)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "cv_scores = cross_val_score(clf, X_train_tfidf, y_train, cv=5) # cross validation\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy: {:.2f}%\".format(cv_scores.mean() * 100))\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 54.15%\n",
      "Classification Report on the test dataset:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.54      0.46      2106\n",
      "           4       0.67      0.54      0.60      3685\n",
      "\n",
      "    accuracy                           0.54      5791\n",
      "   macro avg       0.54      0.54      0.53      5791\n",
      "weighted avg       0.57      0.54      0.55      5791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df['text'] = test_df['text'].str.lower()\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "y_test_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(test_df['target'], y_test_pred)\n",
    "classification_rep = classification_report(test_df['target'], y_test_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report on the test dataset:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Cornell\\course\\CS6386\\Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements\\Sentiment_Analysis\\models_steven.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(output) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     label_scores \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39;49m] \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     label_with_max_score \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(label_scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     predicted_label \u001b[39m=\u001b[39m label_with_max_score[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Blocked by free usage limit (reset hourly)\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/ahmedrachid/FinancialBERT-Sentiment-Analysis\" # original\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tarnformnet/Stock-Sentiment-Bert\" # fine-tuned version\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "predictions = []\n",
    "for text in test_df['text'][209:251]:\n",
    "    output = query({\n",
    "        \"inputs\": text\n",
    "    })\n",
    "    print(output)\n",
    "    if len(output) > 0:\n",
    "        label_scores = output[0] \n",
    "        label_with_max_score = max(label_scores, key=lambda x: x['score']) \n",
    "        predicted_label = label_with_max_score['label']\n",
    "        \n",
    "        if predicted_label == 'Bullish':\n",
    "            predictions.append(4)\n",
    "        elif predicted_label == 'Bearish':\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "\n",
    "test_df['predicted_label'] = predictions\n",
    "\n",
    "accuracy = accuracy_score(test_df['target'], test_df['predicted_label'])\n",
    "classification_rep = classification_report(test_df['target'], test_df['predicted_label'])\n",
    "\n",
    "print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report on the test dataset:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
