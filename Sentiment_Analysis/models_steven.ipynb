{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible datasets\n",
    " - https://data.world/crowdflower/apple-twitter-sentiment\n",
    " - https://paperswithcode.com/dataset/stocknet-1\n",
    " - https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction\n",
    " - https://www.kaggle.com/datasets/thedevastator/tweet-sentiment-s-impact-on-stock-returns\n",
    " - https://ieee-dataport.org/open-access/stock-market-tweets-data\n",
    " - https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset\n",
    " - https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import kaggle\n",
    "\n",
    "import requests\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2023\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.8\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "SEQUENCE_LENGTH = 300\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "W2V_SIZE = 300\n",
    "NUM_CORE = multiprocessing.cpu_count()\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    {\n",
    "        'name': \"training.1600000.processed.noemoticon.csv\",\n",
    "        'api': \"kazanova/sentiment140\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/kazanova/sentiment140\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"reddit_wsb.csv\",\n",
    "        'api': \"gpreda/reddit-wallstreetsbets-posts\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_data.csv\",\n",
    "        'api': \"yash612/stockmarket-sentiment-dataset\",\n",
    "        'location': \"data\",\n",
    "        'url': \"https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stock_tweets.csv\",\n",
    "        'api': \"equinxx/stock-tweets-for-sentiment-analysis-and-prediction\",\n",
    "        'location': \"data/unorganized/Stock Tweets for Sentiment Analysis and Prediction\",\n",
    "        'url': \"https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"Company_Tweet.csv\",\n",
    "        'api': \"omermetinn/tweets-about-the-top-companies-from-2015-to-2020\",\n",
    "        'location': \"data/unorganized/Tweets about the Top Companies from 2015 to 2020\",\n",
    "        'url': \"https://www.kaggle.com/datasets/omermetinn/tweets-about-the-top-companies-from-2015-to-2020\"\n",
    "    },\n",
    "    {\n",
    "        'name': \"stockerbot-export.csv\",\n",
    "        'api': \"davidwallach/financial-tweets\",\n",
    "        'location': \"data/unorganized/Financial Tweets\",\n",
    "        'url': \"https://www.kaggle.com/datasets/davidwallach/financial-tweets\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for dataset_info in DATASETS:\n",
    "    dataset_name = dataset_info['name']\n",
    "    dataset_location = dataset_info['location']\n",
    "\n",
    "    if not os.path.exists(os.path.join(dataset_info['location'], dataset_name)):\n",
    "        print(f\"Downloading {dataset_name} from {dataset_info['url']} to {dataset_location}...\")\n",
    "        kaggle.api.dataset_download_files(dataset_info['api'], path=dataset_location, unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n",
      "target\n",
      "4    3685\n",
      "0    2106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_filename = {\n",
    "    '0': (\"training.1600000.processed.noemoticon.csv\", [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]),\n",
    "    '1': (\"stock_data.csv\", [\"text\", \"target\"])\n",
    "}\n",
    "\n",
    "dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"0\"][0])\n",
    "df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"0\"][1])\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "test_dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"1\"][0])\n",
    "test_df = pd.read_csv(test_dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"1\"][1], skiprows=1)\n",
    "test_df['target'] = test_df['target'].replace({-1: 0, 1: 4})\n",
    "print(test_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...       4\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...       4\n",
       "2  user I'd be afraid to short AMZN - they are lo...       4\n",
       "3                                  MNTA Over 12.00         4\n",
       "4                                   OI  Over 21.37         4\n",
       "5                                  PGNX  Over 3.04         4\n",
       "6  AAP - user if so then the current downtrend wi...       0\n",
       "7  Monday's relative weakness. NYX WIN TIE TAP IC...       0\n",
       "8  GOOG - ower trend line channel test & volume s...       4\n",
       "9             AAP will watch tomorrow for ONG entry.       4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text):\n",
    "#     text = text.lower()\n",
    "\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text) # remove special characters\n",
    "\n",
    "#     stop_words = set(stopwords.words('english')) # tokenization and remove stopwords\n",
    "#     words = text.split()\n",
    "#     words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "#     stemmer = PorterStemmer()\n",
    "#     words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "#     return ' '.join(words)\n",
    "\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "    # text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text)).strip()\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    if stem:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess)\n",
    "test_df['text'] = test_df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>Awww bummer You shoulda got David Carr Third D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>upset update Facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>I dived many times ball Managed save 50 The re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>behaving mad I see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>hey long time see Yes Rains bit bit LOL I fine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>nope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>que muera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  Awww bummer You shoulda got David Carr Third D...  \n",
       "1    scotthamilton  upset update Facebook texting might cry result...  \n",
       "2         mattycus  I dived many times ball Managed save 50 The re...  \n",
       "3          ElleCTF                   whole body feels itchy like fire  \n",
       "4           Karoli                                 behaving mad I see  \n",
       "5         joy_wolf                                         whole crew  \n",
       "6          mybirch                                           Need hug  \n",
       "7             coZZ  hey long time see Yes Rains bit bit LOL I fine...  \n",
       "8  2Hood4Hollywood                                               nope  \n",
       "9          mimismo                                          que muera  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.75472656 0.75392969 0.75528125 0.75476562 0.75523828]\n",
      "Mean CV Accuracy: 75.48%\n",
      "Accuracy: 75.53%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76    159494\n",
      "           4       0.76      0.75      0.75    160506\n",
      "\n",
      "    accuracy                           0.76    320000\n",
      "   macro avg       0.76      0.76      0.76    320000\n",
      "weighted avg       0.76      0.76      0.76    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "alpha = 1  # Laplace smoothing parameter (add-one smoothing), regularization\n",
    "clf = MultinomialNB(alpha=alpha)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "cv_scores = cross_val_score(clf, X_train_tfidf, y_train, cv=5) # cross validation\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy: {:.2f}%\".format(cv_scores.mean() * 100))\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 54.15%\n",
      "Classification Report on the test dataset:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.54      0.46      2106\n",
      "           4       0.67      0.54      0.60      3685\n",
      "\n",
      "    accuracy                           0.54      5791\n",
      "   macro avg       0.54      0.54      0.53      5791\n",
      "weighted avg       0.57      0.54      0.55      5791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df['text'] = test_df['text'].str.lower()\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "y_test_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(test_df['target'], y_test_pred)\n",
    "classification_rep = classification_report(test_df['target'], y_test_pred)\n",
    "\n",
    "print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report on the test dataset:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 1 https://huggingface.co/tarnformnet/Stock-Sentiment-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dff = test_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'Bullish', 'score': 0.6564140319824219}, {'label': 'Bearish', 'score': 0.3435859680175781}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5170243382453918}, {'label': 'Bearish', 'score': 0.48297563195228577}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5612850189208984}, {'label': 'Bearish', 'score': 0.43871498107910156}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5872948169708252}, {'label': 'Bearish', 'score': 0.4127051830291748}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5063714981079102}, {'label': 'Bullish', 'score': 0.49362853169441223}]]\n",
      "[[{'label': 'Bullish', 'score': 0.7190166115760803}, {'label': 'Bearish', 'score': 0.2809833586215973}]]\n",
      "[[{'label': 'Bullish', 'score': 0.601069450378418}, {'label': 'Bearish', 'score': 0.39893051981925964}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6792784333229065}, {'label': 'Bearish', 'score': 0.3207215368747711}]]\n",
      "[[{'label': 'Bullish', 'score': 0.705613911151886}, {'label': 'Bearish', 'score': 0.29438602924346924}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6299093961715698}, {'label': 'Bearish', 'score': 0.3700906038284302}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6812794208526611}, {'label': 'Bearish', 'score': 0.3187205493450165}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6020909547805786}, {'label': 'Bearish', 'score': 0.397909015417099}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5178282260894775}, {'label': 'Bullish', 'score': 0.4821717441082001}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6461464762687683}, {'label': 'Bearish', 'score': 0.35385358333587646}]]\n",
      "[[{'label': 'Bullish', 'score': 0.8016766905784607}, {'label': 'Bearish', 'score': 0.1983233094215393}]]\n",
      "[[{'label': 'Bullish', 'score': 0.7108865976333618}, {'label': 'Bearish', 'score': 0.28911346197128296}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5850090384483337}, {'label': 'Bullish', 'score': 0.41499096155166626}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5397303700447083}, {'label': 'Bullish', 'score': 0.46026960015296936}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5926405787467957}, {'label': 'Bearish', 'score': 0.40735945105552673}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5370607972145081}, {'label': 'Bearish', 'score': 0.4629392623901367}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5995357632637024}, {'label': 'Bearish', 'score': 0.4004642367362976}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5035239458084106}, {'label': 'Bullish', 'score': 0.49647605419158936}]]\n",
      "[[{'label': 'Bearish', 'score': 0.6309901475906372}, {'label': 'Bullish', 'score': 0.3690098226070404}]]\n",
      "[[{'label': 'Bullish', 'score': 0.603463351726532}, {'label': 'Bearish', 'score': 0.396536648273468}]]\n",
      "[[{'label': 'Bullish', 'score': 0.661231517791748}, {'label': 'Bearish', 'score': 0.33876851201057434}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5274107456207275}, {'label': 'Bearish', 'score': 0.47258925437927246}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5448397994041443}, {'label': 'Bearish', 'score': 0.4551602005958557}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6921274662017822}, {'label': 'Bearish', 'score': 0.3078725039958954}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5918205976486206}, {'label': 'Bearish', 'score': 0.4081794321537018}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5682827234268188}, {'label': 'Bearish', 'score': 0.43171727657318115}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6282962560653687}, {'label': 'Bearish', 'score': 0.3717038035392761}]]\n",
      "[[{'label': 'Bullish', 'score': 0.642778217792511}, {'label': 'Bearish', 'score': 0.357221782207489}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5251626372337341}, {'label': 'Bearish', 'score': 0.47483736276626587}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6874387264251709}, {'label': 'Bearish', 'score': 0.3125612735748291}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5921972990036011}, {'label': 'Bearish', 'score': 0.4078027009963989}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5047556161880493}, {'label': 'Bullish', 'score': 0.4952443838119507}]]\n",
      "[[{'label': 'Bullish', 'score': 0.8718454837799072}, {'label': 'Bearish', 'score': 0.128154456615448}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5571137070655823}, {'label': 'Bullish', 'score': 0.44288626313209534}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5931713581085205}, {'label': 'Bearish', 'score': 0.4068286716938019}]]\n",
      "[[{'label': 'Bullish', 'score': 0.662002444267273}, {'label': 'Bearish', 'score': 0.33799758553504944}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5583380460739136}, {'label': 'Bullish', 'score': 0.4416619539260864}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5891684889793396}, {'label': 'Bearish', 'score': 0.410831481218338}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5762412548065186}, {'label': 'Bearish', 'score': 0.42375874519348145}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5650923848152161}, {'label': 'Bearish', 'score': 0.43490758538246155}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6167952418327332}, {'label': 'Bearish', 'score': 0.38320478796958923}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5685094594955444}, {'label': 'Bullish', 'score': 0.43149054050445557}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5374016165733337}, {'label': 'Bearish', 'score': 0.46259841322898865}]]\n",
      "[[{'label': 'Bullish', 'score': 0.6136394143104553}, {'label': 'Bearish', 'score': 0.3863605856895447}]]\n",
      "[[{'label': 'Bearish', 'score': 0.5934690237045288}, {'label': 'Bullish', 'score': 0.4065309762954712}]]\n",
      "[[{'label': 'Bullish', 'score': 0.5013943314552307}, {'label': 'Bearish', 'score': 0.4986056983470917}]]\n",
      "Accuracy on the test dataset: 68.00%\n",
      "Classification Report on the test dataset:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.14      0.11         7\n",
      "           4       0.85      0.77      0.80        43\n",
      "\n",
      "    accuracy                           0.68        50\n",
      "   macro avg       0.47      0.46      0.46        50\n",
      "weighted avg       0.74      0.68      0.71        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven\\AppData\\Local\\Temp\\ipykernel_8304\\4138733689.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['predicted_label'] = predictions\n"
     ]
    }
   ],
   "source": [
    "# Blocked by free usage limit (reset hourly)\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/ahmedrachid/FinancialBERT-Sentiment-Analysis\" # original\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tarnformnet/Stock-Sentiment-Bert\" # fine-tuned version\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "predictions = []\n",
    "for text in test_dff['text']:\n",
    "    output = query({\n",
    "        \"inputs\": text\n",
    "    })\n",
    "    print(output)\n",
    "    if len(output) > 0:\n",
    "        label_scores = output[0] \n",
    "        label_with_max_score = max(label_scores, key=lambda x: x['score']) \n",
    "        predicted_label = label_with_max_score['label']\n",
    "        \n",
    "        if predicted_label == 'Bullish':\n",
    "            predictions.append(4)\n",
    "        elif predicted_label == 'Bearish':\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "\n",
    "test_dff['predicted_label'] = predictions\n",
    "\n",
    "accuracy = accuracy_score(test_dff['target'], test_dff['predicted_label'])\n",
    "classification_rep = classification_report(test_dff['target'], test_dff['predicted_label'])\n",
    "\n",
    "print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report on the test dataset:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'neutral', 'score': 0.9519997835159302}, {'label': 'positive', 'score': 0.0291928481310606}, {'label': 'negative', 'score': 0.018807372078299522}]]\n",
      "[[{'label': 'neutral', 'score': 0.9052541255950928}, {'label': 'negative', 'score': 0.05171525105834007}, {'label': 'positive', 'score': 0.04303063824772835}]]\n",
      "[[{'label': 'neutral', 'score': 0.8800838589668274}, {'label': 'negative', 'score': 0.08712009340524673}, {'label': 'positive', 'score': 0.03279609978199005}]]\n",
      "[[{'label': 'neutral', 'score': 0.9003989100456238}, {'label': 'positive', 'score': 0.0749681368470192}, {'label': 'negative', 'score': 0.024632947519421577}]]\n",
      "[[{'label': 'neutral', 'score': 0.805005669593811}, {'label': 'positive', 'score': 0.13037274777889252}, {'label': 'negative', 'score': 0.06462159007787704}]]\n",
      "[[{'label': 'neutral', 'score': 0.8731374740600586}, {'label': 'positive', 'score': 0.11367302387952805}, {'label': 'negative', 'score': 0.013189527206122875}]]\n",
      "[[{'label': 'neutral', 'score': 0.5701296925544739}, {'label': 'negative', 'score': 0.3820078372955322}, {'label': 'positive', 'score': 0.047862425446510315}]]\n",
      "[[{'label': 'negative', 'score': 0.7603931427001953}, {'label': 'neutral', 'score': 0.19201935827732086}, {'label': 'positive', 'score': 0.04758745804429054}]]\n",
      "[[{'label': 'neutral', 'score': 0.9038221836090088}, {'label': 'positive', 'score': 0.08380158245563507}, {'label': 'negative', 'score': 0.012376200407743454}]]\n",
      "[[{'label': 'neutral', 'score': 0.9443116188049316}, {'label': 'positive', 'score': 0.03489140048623085}, {'label': 'negative', 'score': 0.0207970067858696}]]\n",
      "[[{'label': 'neutral', 'score': 0.9176329970359802}, {'label': 'positive', 'score': 0.045132316648960114}, {'label': 'negative', 'score': 0.03723473846912384}]]\n",
      "[[{'label': 'neutral', 'score': 0.7787973284721375}, {'label': 'positive', 'score': 0.13752858340740204}, {'label': 'negative', 'score': 0.08367407321929932}]]\n",
      "[[{'label': 'neutral', 'score': 0.7658466696739197}, {'label': 'positive', 'score': 0.21205474436283112}, {'label': 'negative', 'score': 0.0220986045897007}]]\n",
      "[[{'label': 'positive', 'score': 0.9448112845420837}, {'label': 'neutral', 'score': 0.03337613865733147}, {'label': 'negative', 'score': 0.021812591701745987}]]\n",
      "[[{'label': 'positive', 'score': 0.8304597735404968}, {'label': 'neutral', 'score': 0.0966123566031456}, {'label': 'negative', 'score': 0.0729278177022934}]]\n",
      "[[{'label': 'neutral', 'score': 0.8391410112380981}, {'label': 'positive', 'score': 0.12455960363149643}, {'label': 'negative', 'score': 0.03629937395453453}]]\n",
      "[[{'label': 'neutral', 'score': 0.8766358494758606}, {'label': 'negative', 'score': 0.08313998579978943}, {'label': 'positive', 'score': 0.04022422805428505}]]\n",
      "[[{'label': 'positive', 'score': 0.8745205998420715}, {'label': 'negative', 'score': 0.07649786025285721}, {'label': 'neutral', 'score': 0.04898153617978096}]]\n",
      "[[{'label': 'neutral', 'score': 0.9198053479194641}, {'label': 'positive', 'score': 0.06695280969142914}, {'label': 'negative', 'score': 0.013241873122751713}]]\n",
      "[[{'label': 'neutral', 'score': 0.8881414532661438}, {'label': 'negative', 'score': 0.08277760446071625}, {'label': 'positive', 'score': 0.029080944135785103}]]\n",
      "[[{'label': 'neutral', 'score': 0.8437207937240601}, {'label': 'positive', 'score': 0.14007334411144257}, {'label': 'negative', 'score': 0.01620585098862648}]]\n",
      "[[{'label': 'positive', 'score': 0.4798559844493866}, {'label': 'neutral', 'score': 0.42818623781204224}, {'label': 'negative', 'score': 0.09195780754089355}]]\n",
      "[[{'label': 'neutral', 'score': 0.882053017616272}, {'label': 'positive', 'score': 0.06604395806789398}, {'label': 'negative', 'score': 0.051903076469898224}]]\n",
      "[[{'label': 'negative', 'score': 0.9343154430389404}, {'label': 'neutral', 'score': 0.05233633145689964}, {'label': 'positive', 'score': 0.013348186388611794}]]\n",
      "[[{'label': 'neutral', 'score': 0.9032260179519653}, {'label': 'positive', 'score': 0.07659651339054108}, {'label': 'negative', 'score': 0.020177436992526054}]]\n",
      "[[{'label': 'neutral', 'score': 0.8024938106536865}, {'label': 'negative', 'score': 0.10980191081762314}, {'label': 'positive', 'score': 0.08770431578159332}]]\n",
      "[[{'label': 'negative', 'score': 0.6496669054031372}, {'label': 'neutral', 'score': 0.3086056709289551}, {'label': 'positive', 'score': 0.041727423667907715}]]\n",
      "[[{'label': 'neutral', 'score': 0.8918415307998657}, {'label': 'negative', 'score': 0.06302031874656677}, {'label': 'positive', 'score': 0.045138198882341385}]]\n",
      "[[{'label': 'neutral', 'score': 0.7184800505638123}, {'label': 'positive', 'score': 0.2697916626930237}, {'label': 'negative', 'score': 0.011728259734809399}]]\n",
      "[[{'label': 'positive', 'score': 0.643550455570221}, {'label': 'negative', 'score': 0.224154993891716}, {'label': 'neutral', 'score': 0.13229455053806305}]]\n",
      "[[{'label': 'positive', 'score': 0.8944012522697449}, {'label': 'neutral', 'score': 0.09723055362701416}, {'label': 'negative', 'score': 0.008368248119950294}]]\n",
      "[[{'label': 'positive', 'score': 0.7691259384155273}, {'label': 'negative', 'score': 0.12108215689659119}, {'label': 'neutral', 'score': 0.10979188233613968}]]\n",
      "[[{'label': 'neutral', 'score': 0.6926836967468262}, {'label': 'positive', 'score': 0.28949618339538574}, {'label': 'negative', 'score': 0.017820091918110847}]]\n",
      "[[{'label': 'positive', 'score': 0.638590395450592}, {'label': 'neutral', 'score': 0.21557265520095825}, {'label': 'negative', 'score': 0.1458369791507721}]]\n",
      "[[{'label': 'positive', 'score': 0.5981854200363159}, {'label': 'negative', 'score': 0.3381502628326416}, {'label': 'neutral', 'score': 0.06366436183452606}]]\n",
      "[[{'label': 'neutral', 'score': 0.8952175378799438}, {'label': 'positive', 'score': 0.08118341863155365}, {'label': 'negative', 'score': 0.023599078878760338}]]\n",
      "[[{'label': 'neutral', 'score': 0.46375176310539246}, {'label': 'positive', 'score': 0.41428327560424805}, {'label': 'negative', 'score': 0.12196499109268188}]]\n",
      "[[{'label': 'neutral', 'score': 0.8774824738502502}, {'label': 'positive', 'score': 0.09918292611837387}, {'label': 'negative', 'score': 0.0233345665037632}]]\n",
      "[[{'label': 'neutral', 'score': 0.8570780754089355}, {'label': 'negative', 'score': 0.10697993636131287}, {'label': 'positive', 'score': 0.03594197705388069}]]\n",
      "[[{'label': 'neutral', 'score': 0.6669798493385315}, {'label': 'positive', 'score': 0.2432211935520172}, {'label': 'negative', 'score': 0.08979891985654831}]]\n",
      "[[{'label': 'neutral', 'score': 0.8666598796844482}, {'label': 'negative', 'score': 0.10582783073186874}, {'label': 'positive', 'score': 0.027512311935424805}]]\n",
      "[[{'label': 'positive', 'score': 0.36343035101890564}, {'label': 'negative', 'score': 0.32260802388191223}, {'label': 'neutral', 'score': 0.3139616549015045}]]\n",
      "[[{'label': 'neutral', 'score': 0.6773326992988586}, {'label': 'positive', 'score': 0.30306383967399597}, {'label': 'negative', 'score': 0.01960347592830658}]]\n",
      "[[{'label': 'neutral', 'score': 0.7653222680091858}, {'label': 'positive', 'score': 0.22334915399551392}, {'label': 'negative', 'score': 0.011328567750751972}]]\n",
      "[[{'label': 'neutral', 'score': 0.9400266408920288}, {'label': 'positive', 'score': 0.036896295845508575}, {'label': 'negative', 'score': 0.023077011108398438}]]\n",
      "[[{'label': 'positive', 'score': 0.6271941065788269}, {'label': 'neutral', 'score': 0.21277360618114471}, {'label': 'negative', 'score': 0.16003228724002838}]]\n",
      "[[{'label': 'positive', 'score': 0.8042299151420593}, {'label': 'negative', 'score': 0.1686042994260788}, {'label': 'neutral', 'score': 0.027165677398443222}]]\n",
      "[[{'label': 'neutral', 'score': 0.725204586982727}, {'label': 'negative', 'score': 0.20842593908309937}, {'label': 'positive', 'score': 0.06636947393417358}]]\n",
      "[[{'label': 'neutral', 'score': 0.6044865250587463}, {'label': 'positive', 'score': 0.37598463892936707}, {'label': 'negative', 'score': 0.01952887699007988}]]\n",
      "[[{'label': 'positive', 'score': 0.7329080104827881}, {'label': 'neutral', 'score': 0.21194177865982056}, {'label': 'negative', 'score': 0.05515025183558464}]]\n",
      "[[{'label': 'neutral', 'score': 0.9161194562911987}, {'label': 'positive', 'score': 0.060656726360321045}, {'label': 'negative', 'score': 0.02322380430996418}]]\n",
      "[[{'label': 'positive', 'score': 0.5267245173454285}, {'label': 'neutral', 'score': 0.384027361869812}, {'label': 'negative', 'score': 0.08924812823534012}]]\n",
      "[[{'label': 'neutral', 'score': 0.795633852481842}, {'label': 'negative', 'score': 0.14014682173728943}, {'label': 'positive', 'score': 0.06421937048435211}]]\n",
      "[[{'label': 'neutral', 'score': 0.5993322730064392}, {'label': 'positive', 'score': 0.3594422936439514}, {'label': 'negative', 'score': 0.04122542217373848}]]\n",
      "[[{'label': 'negative', 'score': 0.6248214244842529}, {'label': 'neutral', 'score': 0.34403088688850403}, {'label': 'positive', 'score': 0.03114771656692028}]]\n",
      "[[{'label': 'neutral', 'score': 0.9367265105247498}, {'label': 'positive', 'score': 0.03366571292281151}, {'label': 'negative', 'score': 0.029607776552438736}]]\n",
      "[[{'label': 'neutral', 'score': 0.8998305201530457}, {'label': 'positive', 'score': 0.07830114662647247}, {'label': 'negative', 'score': 0.021868372336030006}]]\n",
      "[[{'label': 'neutral', 'score': 0.5055745244026184}, {'label': 'positive', 'score': 0.4729539752006531}, {'label': 'negative', 'score': 0.021471485495567322}]]\n",
      "[[{'label': 'neutral', 'score': 0.7740733623504639}, {'label': 'positive', 'score': 0.16231633722782135}, {'label': 'negative', 'score': 0.06361027806997299}]]\n",
      "[[{'label': 'negative', 'score': 0.9745468497276306}, {'label': 'neutral', 'score': 0.015911497175693512}, {'label': 'positive', 'score': 0.009541741572320461}]]\n",
      "[[{'label': 'positive', 'score': 0.9513730406761169}, {'label': 'neutral', 'score': 0.026066796854138374}, {'label': 'negative', 'score': 0.02256017178297043}]]\n",
      "[[{'label': 'neutral', 'score': 0.6465399861335754}, {'label': 'negative', 'score': 0.3358212411403656}, {'label': 'positive', 'score': 0.0176387969404459}]]\n",
      "[[{'label': 'neutral', 'score': 0.8893420696258545}, {'label': 'positive', 'score': 0.09598138928413391}, {'label': 'negative', 'score': 0.014676570892333984}]]\n",
      "[[{'label': 'neutral', 'score': 0.6752782464027405}, {'label': 'positive', 'score': 0.19255514442920685}, {'label': 'negative', 'score': 0.13216662406921387}]]\n",
      "[[{'label': 'neutral', 'score': 0.8473773002624512}, {'label': 'positive', 'score': 0.0856752023100853}, {'label': 'negative', 'score': 0.06694748252630234}]]\n",
      "[[{'label': 'neutral', 'score': 0.9090754389762878}, {'label': 'positive', 'score': 0.05315675586462021}, {'label': 'negative', 'score': 0.03776782006025314}]]\n",
      "[[{'label': 'neutral', 'score': 0.9379706382751465}, {'label': 'positive', 'score': 0.04062904044985771}, {'label': 'negative', 'score': 0.021400293335318565}]]\n",
      "[[{'label': 'neutral', 'score': 0.8695704340934753}, {'label': 'positive', 'score': 0.08005958050489426}, {'label': 'negative', 'score': 0.05036991089582443}]]\n",
      "[[{'label': 'positive', 'score': 0.5016197562217712}, {'label': 'negative', 'score': 0.40589967370033264}, {'label': 'neutral', 'score': 0.09248054772615433}]]\n",
      "[[{'label': 'positive', 'score': 0.9369003772735596}, {'label': 'neutral', 'score': 0.047039151191711426}, {'label': 'negative', 'score': 0.016060370951890945}]]\n",
      "[[{'label': 'neutral', 'score': 0.7934742569923401}, {'label': 'positive', 'score': 0.19608421623706818}, {'label': 'negative', 'score': 0.010441540740430355}]]\n",
      "[[{'label': 'neutral', 'score': 0.8527365326881409}, {'label': 'positive', 'score': 0.13863493502140045}, {'label': 'negative', 'score': 0.008628565818071365}]]\n",
      "[[{'label': 'neutral', 'score': 0.932185709476471}, {'label': 'positive', 'score': 0.047643329948186874}, {'label': 'negative', 'score': 0.020170897245407104}]]\n",
      "[[{'label': 'positive', 'score': 0.7201765775680542}, {'label': 'neutral', 'score': 0.263479083776474}, {'label': 'negative', 'score': 0.016344351693987846}]]\n",
      "[[{'label': 'neutral', 'score': 0.9401082992553711}, {'label': 'negative', 'score': 0.03486796095967293}, {'label': 'positive', 'score': 0.02502378635108471}]]\n",
      "[[{'label': 'neutral', 'score': 0.8891655206680298}, {'label': 'positive', 'score': 0.09578340500593185}, {'label': 'negative', 'score': 0.015051095746457577}]]\n",
      "[[{'label': 'negative', 'score': 0.8188656568527222}, {'label': 'neutral', 'score': 0.15367387235164642}, {'label': 'positive', 'score': 0.027460427954792976}]]\n",
      "[[{'label': 'positive', 'score': 0.8212549686431885}, {'label': 'neutral', 'score': 0.15852662920951843}, {'label': 'negative', 'score': 0.02021840587258339}]]\n",
      "[[{'label': 'neutral', 'score': 0.7570015788078308}, {'label': 'negative', 'score': 0.1495385617017746}, {'label': 'positive', 'score': 0.09345986694097519}]]\n",
      "[[{'label': 'positive', 'score': 0.9291738867759705}, {'label': 'negative', 'score': 0.037573039531707764}, {'label': 'neutral', 'score': 0.03325313702225685}]]\n",
      "[[{'label': 'positive', 'score': 0.5005286335945129}, {'label': 'neutral', 'score': 0.48522520065307617}, {'label': 'negative', 'score': 0.014246147125959396}]]\n",
      "[[{'label': 'neutral', 'score': 0.9421248435974121}, {'label': 'positive', 'score': 0.0314500592648983}, {'label': 'negative', 'score': 0.026425063610076904}]]\n",
      "[[{'label': 'neutral', 'score': 0.9380800127983093}, {'label': 'negative', 'score': 0.04029031842947006}, {'label': 'positive', 'score': 0.02162969298660755}]]\n",
      "[[{'label': 'negative', 'score': 0.8470600843429565}, {'label': 'neutral', 'score': 0.10570041090250015}, {'label': 'positive', 'score': 0.04723949357867241}]]\n",
      "[[{'label': 'neutral', 'score': 0.9332032799720764}, {'label': 'positive', 'score': 0.03556874021887779}, {'label': 'negative', 'score': 0.031227994710206985}]]\n",
      "[[{'label': 'positive', 'score': 0.7369686365127563}, {'label': 'negative', 'score': 0.1758648008108139}, {'label': 'neutral', 'score': 0.08716657757759094}]]\n",
      "[[{'label': 'positive', 'score': 0.8043953776359558}, {'label': 'neutral', 'score': 0.15560264885425568}, {'label': 'negative', 'score': 0.04000191390514374}]]\n",
      "[[{'label': 'neutral', 'score': 0.6681357622146606}, {'label': 'positive', 'score': 0.29717105627059937}, {'label': 'negative', 'score': 0.03469322994351387}]]\n",
      "[[{'label': 'positive', 'score': 0.7121116518974304}, {'label': 'neutral', 'score': 0.23982688784599304}, {'label': 'negative', 'score': 0.04806149750947952}]]\n",
      "[[{'label': 'neutral', 'score': 0.8635128140449524}, {'label': 'positive', 'score': 0.11702803522348404}, {'label': 'negative', 'score': 0.01945912465453148}]]\n",
      "[[{'label': 'neutral', 'score': 0.9106763601303101}, {'label': 'negative', 'score': 0.05449063330888748}, {'label': 'positive', 'score': 0.03483300283551216}]]\n",
      "[[{'label': 'positive', 'score': 0.4645536541938782}, {'label': 'negative', 'score': 0.3187796473503113}, {'label': 'neutral', 'score': 0.21666663885116577}]]\n",
      "[[{'label': 'neutral', 'score': 0.8705695271492004}, {'label': 'positive', 'score': 0.11341039836406708}, {'label': 'negative', 'score': 0.016020065173506737}]]\n",
      "[[{'label': 'neutral', 'score': 0.8605036735534668}, {'label': 'negative', 'score': 0.10202112048864365}, {'label': 'positive', 'score': 0.037475213408470154}]]\n",
      "[[{'label': 'neutral', 'score': 0.8160005211830139}, {'label': 'positive', 'score': 0.14807039499282837}, {'label': 'negative', 'score': 0.035929035395383835}]]\n",
      "[[{'label': 'negative', 'score': 0.6053034067153931}, {'label': 'neutral', 'score': 0.3644000291824341}, {'label': 'positive', 'score': 0.030296552926301956}]]\n",
      "[[{'label': 'neutral', 'score': 0.7053202390670776}, {'label': 'negative', 'score': 0.2704148590564728}, {'label': 'positive', 'score': 0.02426489070057869}]]\n",
      "[[{'label': 'negative', 'score': 0.6506609916687012}, {'label': 'neutral', 'score': 0.2871401906013489}, {'label': 'positive', 'score': 0.06219878047704697}]]\n",
      "[[{'label': 'neutral', 'score': 0.9096413850784302}, {'label': 'positive', 'score': 0.045886922627687454}, {'label': 'negative', 'score': 0.044471755623817444}]]\n",
      "[[{'label': 'neutral', 'score': 0.8497787714004517}, {'label': 'positive', 'score': 0.10098587721586227}, {'label': 'negative', 'score': 0.04923538863658905}]]\n",
      "Accuracy on the test dataset: 27.00%\n",
      "Classification Report on the test dataset:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         0\n",
      "           0       0.56      0.38      0.45        13\n",
      "           4       0.88      0.25      0.39        87\n",
      "\n",
      "    accuracy                           0.27       100\n",
      "   macro avg       0.48      0.21      0.28       100\n",
      "weighted avg       0.84      0.27      0.40       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steven\\AppData\\Local\\Temp\\ipykernel_13584\\1455091469.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dff['predicted_label'] = predictions\n",
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/ProsusAI/finbert\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "predictions = []\n",
    "for text in test_dff['text']:\n",
    "    output = query({\n",
    "        \"inputs\": text\n",
    "    })\n",
    "    print(output)\n",
    "    if len(output) > 0:\n",
    "        label_scores = output[0] \n",
    "        label_with_max_score = max(label_scores, key=lambda x: x['score']) \n",
    "        predicted_label = label_with_max_score['label']\n",
    "        \n",
    "        if predicted_label == 'positive':\n",
    "            predictions.append(4)\n",
    "        elif predicted_label == 'negative':\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "\n",
    "test_dff['predicted_label'] = predictions\n",
    "\n",
    "accuracy = accuracy_score(test_dff['target'], test_dff['predicted_label'])\n",
    "classification_rep = classification_report(test_dff['target'], test_dff['predicted_label'])\n",
    "\n",
    "print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report on the test dataset:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 2   https://github.com/yya518/FinBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" Contemporary Accounting Research (2022).\n",
    "\n",
    "Yang, Yi, Mark Christopher Siy Uy, and Allen Huang. \"Finbert: A pretrained language model for financial communications.\" arXiv preprint arXiv:2006.08097 (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 68.30%\n",
      "Classification Report on the test dataset:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.21      0.32      2106\n",
      "           4       0.68      0.96      0.79      3685\n",
      "\n",
      "    accuracy                           0.68      5791\n",
      "   macro avg       0.70      0.58      0.56      5791\n",
      "weighted avg       0.70      0.68      0.62      5791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3) # to have num_labels = 2, transfer learning by retrain the model using new dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "labels = {0: 4, 1: 4, 2: 0}  # original labels = {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
    "\n",
    "predictions = []\n",
    "for text in test_df['text']:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    outputs = finbert(**inputs)[0]\n",
    "    predicted_label = labels[np.argmax(outputs.detach().numpy())]\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "test_df['predicted_label'] = predictions\n",
    "\n",
    "accuracy = accuracy_score(test_df['target'], test_df['predicted_label'])\n",
    "classification_rep = classification_report(test_df['target'], test_df['predicted_label'])\n",
    "\n",
    "print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report on the test dataset:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 3 https://github.com/ProsusAI/finBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 4 train bert-base-uncased frome the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "config.num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "model.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 5 https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/stefan-it/turkish-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-turkish-128k-uncased')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")\n",
    "\n",
    "# labels = {0: 4, 1: 4, 2: 0} # to change\n",
    "\n",
    "# predictions = []\n",
    "# for text in test_df['text'][:10]:\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "#     outputs = model(**inputs)[0]\n",
    "#     print(outputs)\n",
    "#     print(np.argmax(outputs.detach().numpy()))\n",
    "#     predicted_label = labels[np.argmax(outputs.detach().numpy())]\n",
    "#     predictions.append(predicted_label)\n",
    "\n",
    "# test_df['predicted_label'] = predictions\n",
    "\n",
    "# accuracy = accuracy_score(test_df['target'], test_df['predicted_label'])\n",
    "# classification_rep = classification_report(test_df['target'], test_df['predicted_label'])\n",
    "\n",
    "# print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "# print(\"Classification Report on the test dataset:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/tarnformnet/Stock-Sentiment-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset: 41.65%\n",
      "Classification Report on the test dataset:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.83      0.51      2106\n",
      "           4       0.65      0.18      0.28      3685\n",
      "\n",
      "    accuracy                           0.42      5791\n",
      "   macro avg       0.51      0.51      0.39      5791\n",
      "weighted avg       0.55      0.42      0.36      5791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tarnformnet/Stock-Sentiment-Bert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tarnformnet/Stock-Sentiment-Bert\", from_tf=True)\n",
    "\n",
    "labels = {0: 4, 1: 0}\n",
    "\n",
    "predictions = []\n",
    "for text in test_df['text']:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)[0]\n",
    "    predicted_label = labels[np.argmax(outputs.detach().numpy())]\n",
    "    # print(predicted_label)\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "test_df['predicted_label'] = predictions\n",
    "\n",
    "accuracy = accuracy_score(test_df['target'], test_df['predicted_label'])\n",
    "classification_rep = classification_report(test_df['target'], test_df['predicted_label'])\n",
    "\n",
    "print(\"Accuracy on the test dataset: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Classification Report on the test dataset:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading ()olve/main/vocab.json: 100%|| 899k/899k [00:00<00:00, 3.23MB/s]\n",
      "c:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Steven\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 5.21MB/s]\n",
      "Downloading ()/main/tokenizer.json: 100%|| 1.36M/1.36M [00:00<00:00, 5.12MB/s]\n",
      "Downloading ()lve/main/config.json: 100%|| 481/481 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 748\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[0;32m    750\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    751\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    755\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(value))\n\u001b[1;32m--> 720\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(value)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1024 bytes.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Cornell\\course\\CS6386\\Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements\\Sentiment_Analysis\\models_steven.ipynb Cell 34\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(text, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39mMAX_SEQ_LEN, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m inputs\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(tokenize_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m test_df[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(tokenize_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Train-test split\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\pandas\\core\\series.py:4753\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4755\u001b[0m         func,\n\u001b[0;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[0;32m   4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1289\u001b[0m )\n\u001b[0;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32md:\\Cornell\\course\\CS6386\\Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements\\Sentiment_Analysis\\models_steven.ipynb Cell 34\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_text\u001b[39m(text):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(text, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49mMAX_SEQ_LEN, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Cornell/course/CS6386/Analyzing-the-Correlation-Between-Retail-Traders--Sentiments-and-Equity-Market-Movements/Sentiment_Analysis/models_steven.ipynb#X53sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m inputs\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2790\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2788\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2789\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2790\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2791\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2792\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2896\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2877\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2878\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2894\u001b[0m     )\n\u001b[0;32m   2895\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2896\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2897\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2898\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2899\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2900\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2901\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2902\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2903\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2904\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2905\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2906\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2907\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2908\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2909\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2910\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2911\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2912\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2913\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2914\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2915\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2969\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2959\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2960\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2961\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2962\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2966\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2967\u001b[0m )\n\u001b[1;32m-> 2969\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2970\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2971\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2972\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2973\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2974\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2975\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2976\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2977\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2978\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2979\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2980\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2981\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2982\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2983\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2984\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2985\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2986\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2987\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2988\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils.py:722\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    719\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    720\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 722\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_for_model(\n\u001b[0;32m    723\u001b[0m     first_ids,\n\u001b[0;32m    724\u001b[0m     pair_ids\u001b[39m=\u001b[39;49msecond_ids,\n\u001b[0;32m    725\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    726\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[0;32m    727\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[0;32m    728\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    729\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    730\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    731\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    732\u001b[0m     prepend_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    733\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    734\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    735\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    736\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    737\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    738\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    739\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3459\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[1;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   3456\u001b[0m \u001b[39mif\u001b[39;00m return_length:\n\u001b[0;32m   3457\u001b[0m     encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 3459\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(\n\u001b[0;32m   3460\u001b[0m     encoded_inputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis\n\u001b[0;32m   3461\u001b[0m )\n\u001b[0;32m   3463\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    219\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[0;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[1;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[1;32mc:\\Users\\Steven\\anaconda3\\envs\\stock\\lib\\site-packages\\transformers\\tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    760\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    761\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    762\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    763\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    765\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    766\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    767\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    768\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    769\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define constants\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "MAX_SEQ_LEN = 128  # Maximum sequence length\n",
    "\n",
    "# Load data\n",
    "dataset_filename = {\n",
    "    '0': (\"training.1600000.processed.noemoticon.csv\", [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]),\n",
    "    '1': (\"stock_data.csv\", [\"text\", \"target\"])\n",
    "}\n",
    "\n",
    "dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"0\"][0])\n",
    "df = pd.read_csv(dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"0\"][1])\n",
    "\n",
    "test_dataset_path = os.path.join(\"\", \"data\", dataset_filename[\"1\"][0])\n",
    "test_df = pd.read_csv(test_dataset_path, encoding=DATASET_ENCODING, names=dataset_filename[\"1\"][1], skiprows=1)\n",
    "test_df['target'] = test_df['target'].replace({-1: 0, 1: 4})\n",
    "\n",
    "# Preprocess data\n",
    "# Tokenization, padding, and conversion to PyTorch tensors\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN, return_tensors='pt', padding='longest')\n",
    "    return inputs\n",
    "\n",
    "df['input_ids'] = df['text'].apply(lambda x: tokenize_text(x)['input_ids'])\n",
    "test_df['input_ids'] = test_df['text'].apply(lambda x: tokenize_text(x)['input_ids'])\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create custom PyTorch Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.data.iloc[index]['input_ids']['input_ids'].squeeze()\n",
    "        target = self.data.iloc[index]['target']\n",
    "        return input_ids, target\n",
    "\n",
    "# Initialize DataLoader for training and validation\n",
    "train_dataset = CustomDataset(train_df)\n",
    "val_dataset = CustomDataset(val_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Define RoBERTa model for sequence classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, targets = batch\n",
    "        outputs = model(inputs).logits\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        total_correct += (predicted == targets).sum().item()\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Accuracy on test data: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 6 traing using Amazon SageMaker https://github.com/huggingface/notebooks/blob/main/sagemaker/14_train_and_push_to_hub/sagemaker-notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # too long, not try\n",
    "# # feature extraction\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "# X_train = tfidf_vectorizer.fit_transform(df['text'])\n",
    "# X_test = tfidf_vectorizer.transform(test_df['text'])\n",
    "# y_train = df['target']\n",
    "# y_test = test_df['target']\n",
    "\n",
    "# rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "# print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy on training data: 0.685049375\n",
      "XGBoost Accuracy on testing data: 0.6140562942496978\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf_vectorizer.fit_transform(df['text'])\n",
    "X_test = tfidf_vectorizer.transform(test_df['text'])\n",
    "y_train = df['target']\n",
    "y_train = y_train.apply(lambda x: 1 if x == 4 else x)\n",
    "y_test = test_df['target']\n",
    "y_test = y_test.apply(lambda x: 1 if x == 4 else x)\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_train)\n",
    "xgb_accuracy = accuracy_score(y_train, xgb_predictions)\n",
    "print(\"XGBoost Accuracy on training data:\", xgb_accuracy)\n",
    "\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "print(\"XGBoost Accuracy on testing data:\", xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
