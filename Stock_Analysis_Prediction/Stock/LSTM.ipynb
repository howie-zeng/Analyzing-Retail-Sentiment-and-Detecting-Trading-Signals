{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import helper \n",
    "from models_stationary import *\n",
    "import pywt\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "random_state = helper.RANDOM_STATE\n",
    "\n",
    "# Define a context manager to temporarily suppress FutureWarnings\n",
    "class SuppressFutureWarnings:\n",
    "    def __enter__(self):\n",
    "        warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        warnings.filterwarnings('default')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched for TSLA\n",
      "Data fetched for AAPL\n",
      "Data fetched for QQQ\n",
      "Data fetched for SPY\n",
      "Data fetched for MSFT\n",
      "Data fetched for AMZN\n",
      "Data fetched for GOOG\n",
      "Data fetched for DIA\n",
      "Data fetched for ^IRX\n"
     ]
    }
   ],
   "source": [
    "STOCKS = [\"TSLA\", \"AAPL\", 'QQQ', \"SPY\", \"MSFT\", \"AMZN\", \"GOOG\", \"DIA\", \"^IRX\"]\n",
    "START_DATE = helper.START_DATE\n",
    "END_DATE = helper.END_DATE\n",
    "stock_data = {}\n",
    "MAs = [5, 10, 20, 50, 100, 200]\n",
    "for stock in STOCKS: \n",
    "    data_path = os.path.join(current_path, \"data\", f\"{stock}_{START_DATE}_{END_DATE}.csv\")\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    if stock != \"^IRX\":\n",
    "        data['RSI'] = helper.compute_rsi(data['Close'])\n",
    "        data['rsi_class'] = helper.compute_rsi_class(data)  # Assuming you have this function in helper\n",
    "        #data['volume_class'] = helper.compute_volume_class(data)  # Add volume analysis\n",
    "        data = helper.calculate_mas(data, MAs, column_name=\"Close\")\n",
    "        data['WVAD'] = helper.calculate_wvad(data, period=14)\n",
    "        data['ROC'] = helper.calculate_roc(data, period=14)\n",
    "        data['MACD'], data['macd_line'], data['signal_line'] = helper.calculate_macd(data, short_window=12, long_window=26, signal_window=9)\n",
    "        data['CCI'] =  helper.calculate_cci(data, period=20)\n",
    "        data['Upper Band'], data['Lower Band'], data['SMA'] = helper.calculate_bollinger_bands(data, window=20, num_std_dev=2)\n",
    "        data['SMI'] = helper.calculate_smi(data, period=14, signal_period=3)\n",
    "        data['ATR'] = helper.calculate_atr(data, period=14)\n",
    "        data[['WVF', 'upperBand', 'rangeHigh', 'WVF_color']] = helper.cm_williams_vix_fix(data['Close'], data['Low'])\n",
    "        data[['Buy_Signal', 'Sell_Signal', 'BB_Upper', 'BB_Lower']] = helper.bollinger_rsi_strategy(data['Close'])\n",
    "        data = helper.on_balance_volume(data)\n",
    "        data = helper.volume_price_trend(data)\n",
    "        data = helper.money_flow_index(data)\n",
    "        data = helper.accumulation_distribution(data)\n",
    "        data = data.dropna()\n",
    "    stock_data[stock] = data\n",
    "    print(f\"Data fetched for {stock}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_data = stock_data['AAPL']\n",
    "def apply_wavelet_transform(data, wavelet='haar'):\n",
    "    transformed_data = pd.DataFrame(index=data.index)\n",
    "    for column in data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[column]):\n",
    "            cA, _ = pywt.dwt(data[column].fillna(0), wavelet)\n",
    "            # Pad the transformed data with zeros to match the original length\n",
    "            padding = np.zeros(len(data) - len(cA))\n",
    "            cA_padded = np.concatenate([cA, padding])\n",
    "            transformed_data[column] = cA_padded\n",
    "        else:\n",
    "            transformed_data[column] = data[column]\n",
    "    return transformed_data\n",
    "\n",
    "apple_data = apply_wavelet_transform(apple_data)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "apple_data = scaler.fit_transform(apple_data.select_dtypes(include=np.number))\n",
    "\n",
    "\n",
    "def create_sequences(data, sequence_length, prediction_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - sequence_length - prediction_length + 1):\n",
    "        x = data[i:(i+sequence_length)]\n",
    "        y = data[(i+sequence_length):(i+sequence_length+prediction_length)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 50  # Number of days in the input sequence\n",
    "prediction_length = 5  # Number of days to predict\n",
    "X, y = create_sequences(apple_data, sequence_length, prediction_length)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_size),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, prediction_length, feature_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, prediction_length * feature_dim)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        predictions = self.linear(lstm_out)\n",
    "        predictions = predictions.view(-1, prediction_length, self.feature_dim)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.08243615325631165\n",
      "Epoch 2/10, Loss: 0.01793174379755084\n",
      "Epoch 3/10, Loss: 0.008028478656963604\n",
      "Epoch 4/10, Loss: 0.007150623038774583\n",
      "Epoch 5/10, Loss: 0.006497936323285103\n",
      "Epoch 6/10, Loss: 0.005642410572163943\n",
      "Epoch 7/10, Loss: 0.0045592204508621515\n",
      "Epoch 8/10, Loss: 0.004046449909077548\n",
      "Epoch 9/10, Loss: 0.003894655601825656\n",
      "Epoch 10/10, Loss: 0.003784273228630787\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "num_epochs = 10\n",
    "sae = StackedAutoEncoder(input_size=X_train.shape[2]).to(device)\n",
    "lstm = LSTM(input_size=32, hidden_layer_size=50, prediction_length=5, feature_dim=y_train.shape[2]).to(device)\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_sae = torch.optim.Adam(sae.parameters(), lr=0.001)\n",
    "optimizer_lstm = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop for SAE\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, _ in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer_sae.zero_grad()\n",
    "        outputs = sae(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer_sae.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0860627170016126\n",
      "Epoch 2/10, Loss: 0.053425830311891506\n",
      "Epoch 3/10, Loss: 0.053018490170560234\n",
      "Epoch 4/10, Loss: 0.052938386252740534\n",
      "Epoch 5/10, Loss: 0.05301993167618426\n",
      "Epoch 6/10, Loss: 0.05293284365680159\n",
      "Epoch 7/10, Loss: 0.052833615007196987\n",
      "Epoch 8/10, Loss: 0.05299926503765874\n",
      "Epoch 9/10, Loss: 0.05300334077782747\n",
      "Epoch 10/10, Loss: 0.052771826615420785\n"
     ]
    }
   ],
   "source": [
    "# Extract features for LSTM\n",
    "X_train_encoded = []\n",
    "for inputs, _ in train_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        encoded = sae.encoder(inputs)\n",
    "        X_train_encoded.append(encoded)\n",
    "\n",
    "# Flatten the encoded features to maintain sequence structure\n",
    "X_train_encoded_flat = torch.cat(X_train_encoded, dim=0)\n",
    "\n",
    "# Create new DataLoader for LSTM training\n",
    "train_encoded_data = TensorDataset(X_train_encoded_flat, y_train_tensor)\n",
    "train_encoded_loader = DataLoader(train_encoded_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training loop for LSTM\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_encoded_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer_lstm.zero_grad()\n",
    "        predictions = lstm(inputs)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer_lstm.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_encoded_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error: 0.03452277183532715\n",
      "Entire Dataset Mean Squared Error: 0.04800975605637996\n"
     ]
    }
   ],
   "source": [
    "def encode_and_predict(data_loader, sae, lstm):\n",
    "    encoded_features = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in data_loader:\n",
    "            inputs = inputs[0].to(device)  # Assuming the data_loader yields only features\n",
    "            encoded = sae.encoder(inputs)\n",
    "            lstm_out = lstm(encoded)\n",
    "            encoded_features.append(encoded.cpu().numpy())\n",
    "            predictions.append(lstm_out.cpu().numpy())\n",
    "    return np.concatenate(encoded_features), np.concatenate(predictions)\n",
    "\n",
    "# Prepare data loaders for the test set and the entire dataset\n",
    "test_encoded_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "all_data_loader = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32)), batch_size=64, shuffle=False)\n",
    "\n",
    "# Encode and predict for the test set and the entire dataset\n",
    "_, test_predictions = encode_and_predict(test_encoded_loader, sae, lstm)\n",
    "_, all_predictions = encode_and_predict(all_data_loader, sae, lstm)\n",
    "\n",
    "test_mse = np.mean((y_test_tensor.cpu().numpy() - test_predictions) ** 2)\n",
    "print(f\"Test Mean Squared Error: {test_mse}\")\n",
    "\n",
    "all_mse = np.mean((y - all_predictions) ** 2)\n",
    "print(f\"Entire Dataset Mean Squared Error: {all_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inverse_wavelet_transform() missing 1 required positional argument: 'sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-16079d882159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0moriginal_test_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minverse_wavelet_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_test_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0moriginal_all_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minverse_wavelet_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_all_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: inverse_wavelet_transform() missing 1 required positional argument: 'sequence_length'"
     ]
    }
   ],
   "source": [
    "def inverse_transform(predictions, scaler, sequence_length):\n",
    "    num_samples, sequence_length, feature_dim = predictions.shape\n",
    "    reshaped_predictions = predictions.reshape(num_samples * sequence_length, feature_dim)\n",
    "    return scaler.inverse_transform(reshaped_predictions)\n",
    "\n",
    "\n",
    "def inverse_wavelet_transform(predictions, sequence_length, wavelet='haar'):\n",
    "    num_samples, feature_dim = predictions.shape\n",
    "    reshaped_predictions = predictions.reshape(num_samples * sequence_length, feature_dim)\n",
    "    \n",
    "    inverted_data = pd.DataFrame(index=range(len(reshaped_predictions)))\n",
    "    for i in range(feature_dim):\n",
    "        inverted_data[i] = pywt.idwt(reshaped_predictions[:, i], None, wavelet)\n",
    "    return inverted_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "original_test_predictions = inverse_transform(test_predictions, scaler, sequence_length)\n",
    "original_all_predictions = inverse_transform(all_predictions, scaler, sequence_length)\n",
    "\n",
    "\n",
    "original_test_predictions = inverse_wavelet_transform(original_test_predictions)\n",
    "original_all_predictions = inverse_wavelet_transform(original_all_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
