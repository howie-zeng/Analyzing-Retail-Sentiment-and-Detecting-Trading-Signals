{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import helper \n",
    "from models_stationary import *\n",
    "import pywt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "random_state = helper.RANDOM_STATE\n",
    "\n",
    "# Define a context manager to temporarily suppress FutureWarnings\n",
    "class SuppressFutureWarnings:\n",
    "    def __enter__(self):\n",
    "        warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        warnings.filterwarnings('default')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched for TSLA\n",
      "Data fetched for AAPL\n",
      "Data fetched for QQQ\n",
      "Data fetched for SPY\n",
      "Data fetched for MSFT\n",
      "Data fetched for AMZN\n",
      "Data fetched for GOOG\n",
      "Data fetched for DIA\n",
      "Data fetched for ^IRX\n"
     ]
    }
   ],
   "source": [
    "STOCKS = [\"TSLA\", \"AAPL\", 'QQQ', \"SPY\", \"MSFT\", \"AMZN\", \"GOOG\", \"DIA\", \"^IRX\"]\n",
    "START_DATE = helper.START_DATE\n",
    "END_DATE = helper.END_DATE\n",
    "stock_data = {}\n",
    "MAs = [5, 10, 20, 50, 100, 200]\n",
    "for stock in STOCKS: \n",
    "    data_path = os.path.join(current_path, \"data\", f\"{stock}_{START_DATE}_{END_DATE}.csv\")\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    if stock != \"^IRX\":\n",
    "        data['RSI'] = helper.compute_rsi(data['Close'])\n",
    "        data['rsi_class'] = helper.compute_rsi_class(data)  # Assuming you have this function in helper\n",
    "        #data['volume_class'] = helper.compute_volume_class(data)  # Add volume analysis\n",
    "        data = helper.calculate_mas(data, MAs, column_name=\"Close\")\n",
    "        data['WVAD'] = helper.calculate_wvad(data, period=14)\n",
    "        data['ROC'] = helper.calculate_roc(data, period=14)\n",
    "        data['MACD'], data['macd_line'], data['signal_line'] = helper.calculate_macd(data, short_window=12, long_window=26, signal_window=9)\n",
    "        data['CCI'] =  helper.calculate_cci(data, period=20)\n",
    "        data['Upper Band'], data['Lower Band'], data['SMA'] = helper.calculate_bollinger_bands(data, window=20, num_std_dev=2)\n",
    "        data['SMI'] = helper.calculate_smi(data, period=14, signal_period=3)\n",
    "        data['ATR'] = helper.calculate_atr(data, period=14)\n",
    "        data[['WVF', 'upperBand', 'rangeHigh', 'WVF_color']] = helper.cm_williams_vix_fix(data['Close'], data['Low'])\n",
    "        data[['Buy_Signal', 'Sell_Signal', 'BB_Upper', 'BB_Lower']] = helper.bollinger_rsi_strategy(data['Close'])\n",
    "        data = helper.on_balance_volume(data)\n",
    "        data = helper.volume_price_trend(data)\n",
    "        data = helper.money_flow_index(data)\n",
    "        data = helper.accumulation_distribution(data)\n",
    "        data = data.dropna()\n",
    "    stock_data[stock] = data\n",
    "    print(f\"Data fetched for {stock}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date is not numerical or is the target variable\n",
      "rsi_class is not numerical or is the target variable\n"
     ]
    }
   ],
   "source": [
    "stock = 'AAPL'\n",
    "df_stock = stock_data[stock].copy()\n",
    "if len(df_stock) %2 != 0:\n",
    "    df_stock = df_stock[:-1]\n",
    "close_prices_Y = df_stock['Close']\n",
    "\n",
    "y_index = df_stock.columns.get_loc('Close')\n",
    "df_stock_swt = helper.apply_stationary_wavelet_transform(df_stock)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_stock_swt = scaler.fit_transform(df_stock_swt)\n",
    "\n",
    "\n",
    "sequence_length = 200  # days in the input sequence\n",
    "prediction_length = 5  # days to predict\n",
    "\n",
    "\n",
    "def create_sequences(input_data, target_data, sequence_length, prediction_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(input_data) - sequence_length - prediction_length + 1):\n",
    "        xs.append(input_data[i:(i + sequence_length)])\n",
    "        ys.append(target_data[(i + sequence_length):(i + sequence_length + prediction_length)])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X, y = create_sequences(df_stock_swt, close_prices_Y, sequence_length, prediction_length)\n",
    "\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "train_size = int(0.8 * len(X_tensor))\n",
    "X_train_tensor = X_tensor[:train_size]\n",
    "y_train_tensor = y_tensor[:train_size]\n",
    "X_test_tensor = X_tensor[train_size:]\n",
    "y_test_tensor = y_tensor[train_size:]\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob, weight_decay):\n",
    "        super(StockPriceLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob, bidirectional=False, bias=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Output size does not depend on bidirectional\n",
    "        \n",
    "        # Add Batch Normalization Layer\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.weight_decay = weight_decay  # Regularization parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # Output shape: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        out = out.transpose(1, 2)  # Adjust for batch normalization\n",
    "        out = self.batchnorm(out)\n",
    "        out = out.transpose(1, 2)  # Adjust back to the original shape\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])  # Output shape: (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0184\n",
      "Epoch [2/50], Loss: 0.0138\n",
      "Epoch [3/50], Loss: 0.0118\n",
      "Epoch [4/50], Loss: 0.0147\n",
      "Epoch [5/50], Loss: 0.0181\n",
      "Epoch [6/50], Loss: 0.0183\n",
      "Epoch [7/50], Loss: 0.0179\n",
      "Epoch [8/50], Loss: 0.0182\n",
      "Epoch [9/50], Loss: 0.0183\n",
      "Epoch [10/50], Loss: 0.0184\n",
      "Epoch [11/50], Loss: 0.0342\n",
      "Epoch [12/50], Loss: 0.0304\n",
      "Epoch [13/50], Loss: 0.0251\n",
      "Epoch [14/50], Loss: 0.0213\n",
      "Epoch [15/50], Loss: 0.0196\n",
      "Epoch [16/50], Loss: 0.0192\n",
      "Epoch [17/50], Loss: 0.0185\n",
      "Epoch [18/50], Loss: 0.0189\n",
      "Epoch [19/50], Loss: 0.0188\n",
      "Epoch [20/50], Loss: 0.0193\n",
      "Epoch [21/50], Loss: 0.0939\n",
      "Epoch [22/50], Loss: 0.0881\n",
      "Epoch [23/50], Loss: 0.0893\n",
      "Epoch [24/50], Loss: 0.0895\n",
      "Epoch [25/50], Loss: 0.0895\n",
      "Epoch [26/50], Loss: 0.0895\n",
      "Epoch [27/50], Loss: 0.0895\n",
      "Epoch [28/50], Loss: 0.0896\n",
      "Epoch [29/50], Loss: 0.0896\n",
      "Epoch [30/50], Loss: 0.0896\n",
      "Epoch [31/50], Loss: 0.0928\n",
      "Epoch [32/50], Loss: 0.0947\n",
      "Epoch [33/50], Loss: 0.0954\n",
      "Epoch [34/50], Loss: 0.0956\n",
      "Epoch [35/50], Loss: 0.0957\n",
      "Epoch [36/50], Loss: 0.0957\n",
      "Epoch [37/50], Loss: 0.0957\n",
      "Epoch [38/50], Loss: 0.0957\n",
      "Epoch [39/50], Loss: 0.0957\n",
      "Epoch [40/50], Loss: 0.0957\n",
      "Epoch [41/50], Loss: 0.0961\n",
      "Epoch [42/50], Loss: 0.0969\n",
      "Epoch [43/50], Loss: 0.0973\n",
      "Epoch [44/50], Loss: 0.0976\n",
      "Epoch [45/50], Loss: 0.0978\n",
      "Epoch [46/50], Loss: 0.0979\n",
      "Epoch [47/50], Loss: 0.0979\n",
      "Epoch [48/50], Loss: 0.0980\n",
      "Epoch [49/50], Loss: 0.0980\n",
      "Epoch [50/50], Loss: 0.0980\n",
      "Average Test Loss: 0.4108\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "input_size = X.shape[2]  # Adjust based on your input features\n",
    "hidden_size = 50\n",
    "num_layers = 3\n",
    "output_size = prediction_length\n",
    "dropout_prob = 0.4\n",
    "weight_decay = 0.001  \n",
    "model = StockPriceLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob, weight_decay).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "    \n",
    "\n",
    "average_loss = np.mean(test_losses)\n",
    "print(f'Average Test Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions (example on the test set)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "# Convert predictions to a single array\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 70)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 70 features, but MinMaxScaler is expecting 5 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-7e986891719f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlast_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_stock_swt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlast_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlast_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Making the prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Howard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Howard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    516\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Howard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Howard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    416\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 70 features, but MinMaxScaler is expecting 5 features as input."
     ]
    }
   ],
   "source": [
    "last_sequence = df_stock_swt[-sequence_length:]\n",
    "last_sequence = scaler.transform(last_sequence)\n",
    "last_sequence = torch.tensor(last_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# Making the prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(last_sequence)\n",
    "    prediction = prediction.cpu().numpy()\n",
    "\n",
    "actual_prices = close_prices_Y[-sequence_length:].values\n",
    "\n",
    "dates = pd.to_datetime(df_stock['Date'].iloc[-sequence_length:])\n",
    "forecast_dates = pd.date_range(start=dates.iloc[-1], periods=len(prediction[0]))  # Adjusted length\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, actual_prices, label='Actual Prices')\n",
    "plt.plot(forecast_dates, prediction[0], label='Predicted Prices', linestyle='--')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
